file server
dynamic host
hugo buildlet



need to check on user registration full flow using code

read roles from api Objects and check roles before cmnd (role auto-checker)
make the rate limiter a configurable api thing
can potentially make a plugin system to hook in 3rd party functionality

group user role assignment automated via invite method.. different invite urls can place users in different subgroups, but first need to set "staff" and "user" role on group


change feedback api to be group centric at the type name level

do the delete lock unlock apis for group_users


3/31 - 22649

TWO PATHS


#1 - i'm creating a writing center to derive functionality

#2 - i'm creating a generic web application generation framework


Can I allow my users to vote on the next thing to be developed?

- limitations, (cant vote on next if voted) (2 week polling)
- expectations, (functionality must be generic) 


Write an explanation around the following with respect to the API:


interface BEEP {
  endpoints: {
    bop: {
      typer: IUserProfile,
      bopper: put
    }
    bap: {
      typer: IUserProfile,
      bopper: post
    }
  }
}

type put = 'PUT'
type post = 'POST'

type Tester<T> = T extends {
  [prop in T as keyof T]: { typer: infer Q, bopper: infer B }
} ? {
  [K in keyof T] : T[K] extends {
    bopper: put
  } ? { foo: 'PUT', bar: Q, baz: B } : T[K] extends {
    bopper: post
  } ? { foo: 'POST', bar: Q, baz: B} : never
} : never;

type Tested = Tester<BEEP['endpoints']> /* {
  bop: {
      foo: 'PUT';
      bar: IUserProfile;
      baz: "PUT" | "POST";
  };
  bap: {
      foo: 'POST';
      bar: IUserProfile;
      baz: "PUT" | "POST";
  };
}
*/




// guided-edit info

This goes into GPT ->

// Profile.tsx

{
  "statement_0": "import bla from..",
  "statement_1": "something",
  "statement_2": "const myVar = 'hey'",
  "statement_3": "something",
  "statement_4": "something",
  "statement_5": "something",
  "statement_6": "useEffect(() => { // some use effect thing }, [])",
  "statement_7": "something",
  "statement_8": "something",
  "statement_9": "something",
  "statement_10": "something",
  "statement_11": "export default whatever;",
}

Profile.tsx hey we need to chagne myvar from myVar to yourVar and it should also say 'yo', not hey. also after that we need to check that yourVar is set to 'yo' just to make sure you didn't mess up


THis comes out of gpt <-

[
  {
    "statement_2": "const yourVar = 'yo';",
    "below_2": "if (yourVar !== 'yo') ..." 
  }
]









Model: GPT-4



The conversation that follows describes how you learn the information necessary to define a novel compression algorithm.

Background: In a previous query you described ééé.

Action: Instead of taking action on the text in ééé, this exercise is to expand on the concepts used in the Problem @@@ and Solution &&& that follow.

ééé
Compress the following text in a way that fits in a tweet (ideally) and such that you (GPT-4) can reconstruct the intention of the human who wrote text as close as possible to the original intention. This is for yourself. It does not need to be human readable or understandable. Abuse of language mixing, abbreviations, symbols (unicode and emoji), or any other encodings or internal representations is all permissible, as long as it, if pasted in a new inference cycle, will yield near-identical results as the original text. Numerical values and names are important, don't remove them.
ééé

Review the following Problem @@@ and Solution &&&, finally, observe ||| to determine the purpose of your response.

Problem @@@
We have a program where an AI-powered assistant performs the following actions on files:
-create
-update
-delete

file system is large (1000+ files)

can't give entire context at once to ai; each file is 3000 - 10000 characters; ai prohibits a context greater than 10000
@@@

Solution &&&
We must compress the nodes of each file in the application by designing a compression algorithm that, while allowing the introduction of new members, all members may decode themselves given their position relative to other nodes that are similar to themselves. Implementation details described in %%%.

%%%
given
  - file A
  - file B
  - file C
  - file D
  - file E
  - ...
  - file 1001

all of these files have similar traits, such as imports, const declartaions, functions, anything that could be analyzed within an abstract syntax tree, etc.

using a monorepo as an example, the tree might span different languages and filetypes, exluding heavy-set objects: package manager's installation dir (node_modules), images, or some other file the developer ignores

given that the file set can be reduced to abstract syntax trees, you describe the process of distilling all nodes of an application, from the input to the application's entirety. distill the entirety of the application's node structure into an algorithm that will allow us to access any child in the application's AST, given the node's unique identifier. the algorithm just requires a single node in order to produce a result (which is effectively of the parent node, or prototypical structure).

Any node's identifier is a function that returns the node's current value in the greater tree. In this way we should be able to utilize the compression techniques described in ééé in the AST paradigm.

Desired algorithmic effect: 
COMPRESSION_SUM = [1...1000 + n]

// some file
import compressionSum from 'path/to/COMPRESSION_SUM';

compressionSum['getCustomProperty']() // ever-generating structure; upon execution infers its value
%%%
&&&

|||
Now that you have reviewed @@@ and &&&, insist on a method and language for implementing the requirements therein.
|||

Based on the problem and solution provided, it is evident that the main goal is to design a compression algorithm for a large file system consisting of 1000+ files, each with 3000-10000 characters. The algorithm should be capable of compressing nodes of each file while allowing the introduction of new members. The compressed files should be easily decoded using their relative positions among similar nodes. It is also important to consider the abstract syntax tree (AST) representation of the files.

To implement the requirements, I would suggest using a combination of Huffman coding and an adaptation of the Burrows-Wheeler Transform (BWT) for the compression process. Huffman coding is a widely-used lossless data compression technique that assigns shorter codes to more frequent characters, while BWT is a data transformation technique that rearranges a string into runs of similar characters.

Here's a high-level overview of the suggested method:

Parse the files into abstract syntax trees (ASTs) using a suitable parser depending on the programming language(s) of the files.
Apply a modified version of the Burrows-Wheeler Transform (BWT) to reorder the nodes of the ASTs based on their similarity.
Compress the reordered ASTs using Huffman coding.
For the implementation, I recommend using Python as the programming language. Python has a rich ecosystem of libraries, including parsers for various languages and data compression algorithms. Additionally, Python is known for its simplicity and readability, which will facilitate the development and maintenance of the compression algorithm.

To get started with the implementation, you can utilize the following Python libraries:

ast or astor: For parsing and manipulating Python abstract syntax trees.
pycparser: For parsing and manipulating C/C++ abstract syntax trees.
huffman: For implementing Huffman coding.
Remember to adapt the Burrows-Wheeler Transform (BWT) for the specific requirements of the problem and to integrate it with the Huffman coding process.




Implement #2 and onward in the context of a typescript project utilizing ts-morph. While you do that, I'm gonna watch a video on Burrows-Wheeler Transform.
Sure, here's a TypeScript implementation utilizing the ts-morph library for parsing and manipulating TypeScript abstract syntax trees. In this example, I'll focus on implementing a modified version of the Burrows-Wheeler Transform (BWT) and applying it to the ASTs.

First, ensure that you have ts-morph and typescript installed in your project:
bash


npm install ts-morph typescript
Create a new TypeScript file named compress.ts and import the necessary dependencies:
typescript


import { Project } from "ts-morph";
Write a function to apply the modified BWT to the ASTs:
typescript


function applyModifiedBWT(project: Project) {
  // Traverse the ASTs and apply the modified BWT
  project.getSourceFiles().forEach((sourceFile) => {
    const imports = sourceFile.getImportDeclarations();
    const functions = sourceFile.getFunctions();
    const variables = sourceFile.getVariableDeclarations();
    const classes = sourceFile.getClasses();

    // Apply the modified BWT to each group of similar nodes
    const reorderedImports = bwtReorder(imports);
    const reorderedFunctions = bwtReorder(functions);
    const reorderedVariables = bwtReorder(variables);
    const reorderedClasses = bwtReorder(classes);

    // Replace the original nodes with the reordered nodes
    replaceNodes(imports, reorderedImports);
    replaceNodes(functions, reorderedFunctions);
    replaceNodes(variables, reorderedVariables);
    replaceNodes(classes, reorderedClasses);
  });
}

function bwtReorder(nodes: any[]) {
  // Implement the modified BWT to reorder nodes based on their similarity
  // This is a placeholder implementation; you should modify it according to your specific requirements
  return nodes.sort((a, b) => a.getText().localeCompare(b.getText()));
}

function replaceNodes(originalNodes: any[], reorderedNodes: any[]) {
  originalNodes.forEach((node, index) => {
    node.replaceWith(reorderedNodes[index]);
  });
}
Now, create an instance of the Project class and call the applyModifiedBWT function:
typescript


const project = new Project({
  compilerOptions: {
    target: "ES2020",
    module: "CommonJS",
  },
});

// Add the source files to the project (you should adjust the file paths accordingly)
project.addSourceFilesAtPaths("path/to/your/source/files/**/*.ts");

// Apply the modified BWT to the ASTs
applyModifiedBWT(project);
Finally, you can apply a compression algorithm such as Huffman coding to the reordered ASTs. Note that the Huffman coding implementation is not included in this example, as the focus here is on the modified BWT.
Remember that this is just a basic example and may require further adaptations to fit your specific use case. You should also investigate an appropriate method to reverse the modified BWT during the decompression process.

Feel free to ask any questions or request further assistance on implementing the Huffman coding or other parts of the compression algorithm.





2 / 2

Show an example Node before and after it has been modified using BWT. Let's say the file is a simple typescript file defining a let and a const string and a function that adds the const to the let.

Then, continue with the Huffman implementation using the exact artifacts you have created in this example, plus whatever else is needed to continue the Huffman example.
First, let's create a simple TypeScript file named example.ts:

typescript


const myConst = "Hello, ";
let myVar = "world!";

function concatStrings(a: string, b: string): string {
  return a + b;
}

console.log(concatStrings(myConst, myVar));
Now, let's modify the compress.ts script to print a node before and after applying the BWT, and then implement Huffman coding to compress the reordered ASTs.

Modify the applyModifiedBWT function to print the nodes before and after applying the BWT:
typescript


function applyModifiedBWT(project: Project) {
  // Traverse the ASTs and apply the modified BWT
  project.getSourceFiles().forEach((sourceFile) => {
    // ... (previous code)

    // For demonstration purposes, print the nodes before and after applying the BWT
    console.log("Before BWT:");
    console.log(functions.map((fn) => fn.getText()).join("\n"));

    // Apply the modified BWT to each group of similar nodes
    // ... (previous code)

    // For demonstration purposes, print the nodes after applying the BWT
    console.log("\nAfter BWT:");
    console.log(reorderedFunctions.map((fn) => fn.getText()).join("\n"));

    // Replace the original nodes with the reordered nodes
    // ... (previous code)
  });
}
Add the example.ts file to the project:
typescript


// Add the source files to the project
project.addSourceFileAtPath("example.ts");

// Apply the modified BWT to the ASTs
applyModifiedBWT(project);
Implement the Huffman coding compression:
typescript


class HuffmanNode {
  public left: HuffmanNode | null = null;
  public right: HuffmanNode | null = null;
  public value: string | null = null;
  public weight: number;

  constructor(weight: number, value?: string) {
    this.weight = weight;
    if (value) this.value = value;
  }
}

function huffmanEncode(source: string): [string, Map<string, string>] {
  const frequencyMap = new Map<string, number>();

  for (const char of source) {
    frequencyMap.set(char, (frequencyMap.get(char) || 0) + 1);
  }

  const nodes = Array.from(frequencyMap.entries()).map(
    ([value, weight]) => new HuffmanNode(weight, value)
  );

  while (nodes.length > 1) {
    nodes.sort((a, b) => a.weight - b.weight);

    const leftNode = nodes.shift() as HuffmanNode;
    const rightNode = nodes.shift() as HuffmanNode;
    const newNode = new HuffmanNode(leftNode.weight + rightNode.weight);

    newNode.left = leftNode;
    newNode.right = rightNode;

    nodes.push(newNode);
  }

  const huffmanTree = nodes[0];
  const encodingMap = new Map<string, string>();

  const traverse = (node: HuffmanNode, path: string) => {
    if (node.value) {
      encodingMap.set(node.value, path);
    } else {
      if (node.left) traverse(node.left, path + "0");
      if (node.right) traverse(node.right, path + "1");
    }
  };

  traverse(huffmanTree, "");

  const encodedString = source.split("").map((char) => encodingMap.get(char)).join("");
  return [encodedString, encodingMap];
}
Apply the Huffman coding compression to the reordered ASTs:
typescript


const sourceFile = project.getSourceFileOrThrow("example.ts");
const encoded

Apply the Huffman coding compression to the reordered ASTs:
typescript


const sourceFile = project.getSourceFileOrThrow("example.ts");
const encoded = huffmanEncode(sourceFile.getFullText());
console.log("\nHuffman encoded text:\n", encoded[0]);
Implement the Huffman decoding function:
typescript


function huffmanDecode(encoded: string, encodingMap: Map<string, string>): string {
  const reverseEncodingMap = new Map<string, string>();

  for (const [key, value] of encodingMap.entries()) {
    reverseEncodingMap.set(value, key);
  }

  let decoded = "";
  let buffer = "";

  for (const bit of encoded) {
    buffer += bit;

    if (reverseEncodingMap.has(buffer)) {
      decoded += reverseEncodingMap.get(buffer);
      buffer = "";
    }
  }

  return decoded;
}

Test the Huffman encoding and decoding process:
typescript


const decoded = huffmanDecode(encoded[0], encoded[1]);
console.log("\nHuffman decoded text:\n", decoded);
Now, running the compress.ts script will apply the modified Burrows-Wheeler Transform to the ASTs, print the function node before and after applying the BWT, and perform Huffman encoding and decoding on the reordered ASTs.

Keep in mind that the BWT implementation provided in this example is a placeholder, and you should modify it according to your specific requirements. Additionally, this example focuses on compressing a single TypeScript file. You can easily extend it to process multiple files by adding more source files to the project and iterating through them in the applyModifiedBWT function.

This is good. Continue to think about the process and improvements. Iterate ten thousand times internally, each time scoring the results on their 













type ConfirmActionProps = [
  approval?: boolean,
]






// convert a string into a unique symbol for the purposes of dynamic attr selection
// const thing = 'myAttr';
// const requestAccessor: ReturnType<typeof sym> = sym(thing);
// const { [requestAccessor]: giveItAName } = someObject;
export const sym: (a: string) => { readonly 0: unique symbol }[0] = a => Symbol(a) as ReturnType<typeof sym>;


const stuff = {
  myKey: 'val'
};

function assignSym(thing: string) {

  const mySym: ReturnType<typeof sym> = sym(thing);

  const {
    [mySym]: someProp
  } = stuff;


}




type MySymbols = {
  [prop: string]: Symbol
}

const mySyms: MySymbols = {
  test: Symbol('test')
}


type SymMapper<T> = T extends Symbol ? Symbol : T extends symbol ? symbol : never; 

type MappedSym<T> = {
  [K in keyof SymMapper<T>]: any
}

type SmallSym = {
  [prop: symbol]: any
}





const symConst: Record<symbol, any> = {
  [Symbol.for('5')]: 'test'
} as const;

const stuff2: MappedSym<typeof symConst> = {
  test: 'yo'
}

const symType: unique symbol = Symbol(5);



function setSymbol(thing: string) {

}


const thingy: Map<Symbol, Record<string, 'dog'>> = new Map();
thingy.set(Symbol(5), { hey: 'dog' });



for (const [thingyChild, rec] of thingy) {
  
  const thingReceived = thingy.get(thingyChild);

}



type SymbolObject = {
  [prop: ReturnType<typeof sym>]: string | (() => Promise<void>) | Array<unknown> | Record<string, unknown>;
}


const obj: SymbolObject = {
  [Symbol('foo')]: "foo",
  [Symbol('bar')]: async () => {},
  foo: [],
  bar: {}
} as const;

function PromiseAll(...args: Awaited<readonly (number | Promise<number | string>)[]>[]) {}

const promise1 = Promise.resolve(3);
const promise2 = 42;
const promise3 = new Promise<string>((resolve, reject) => {
  setTimeout(resolve, 100, 'foo');
});

// expected to be `Promise<[number, 42, string]>`
const p = PromiseAll([promise1, promise2, promise3] as const)



Let's create a simple shell script utility that will run my program and give me an interactive menu to restart a service or stop or start one. Use a debian based utility for all needs, and try to do it natively in all cases.

Each service should be parsed from a configuration object we define at the top, which contains the service name and a start command, like

app=npm run start --target /app/website
db=pg_connect ...

Then our cli starts up, and we see:

Press # of services to start, then press Enter to start.
1. api x
2. app x
3. db
4. etc

An "x" appears to each service if it has been selected. 
When we press Enter, each service starts by running its associated command. 
All logging for all started services is output to local.log.
After starting, we are given a menu that looks like:

Select an action.
1. Stop
2. Start
3. Restart
4. Shutdown

Actions 1-3 allow us to once again select from all the different services, denoted with an x, and perform the desired action.
The Shutdown action safely shuts down all services immediately.


Certbot always puts the latest version of all certificates under /etc/letsencrypt/live:

/etc/letsencrypt/live
├── mail.example.org
│   ├── cert.pem -> ../../archive/mail.example.org/cert8.pem
│   ├── chain.pem -> ../../archive/mail.example.org/chain8.pem
│   ├── fullchain.pem -> ../../archive/mail.example.org/fullchain8.pem
│   └── privkey.pem -> ../../archive/mail.example.org/privkey8.pem
└── www.example.org
    ├── cert.pem -> ../../archive/www.example.org/cert7.pem
    ├── chain.pem -> ../../archive/www.example.org/chain7.pem
    ├── fullchain.pem -> ../../archive/www.example.org/fullchain7.pem
    └── privkey.pem -> ../../archive/www.example.org/privkey7.pem
Therefore you would configure services like this:

SSLCertificateFile     /etc/letsencrypt/live/www.example.org/fullchain.pem
SSLCertificateKeyFile  /etc/letsencrypt/live/www.example.org/privkey.pem


-- Schedule bracket slot relationship example

SELECT 
	count("schedule".*) as schedulesCount,
	count("bracket".*) as bracketsCount,
	count("slot".*) as slotCount,
	count("service".*) as serviceCount
FROM
	dbtable_schema.schedules schedule
	JOIN dbtable_schema.schedule_brackets bracket ON bracket.schedule_id = schedule.id
	JOIN dbtable_schema.schedule_bracket_services "service" ON "service".schedule_bracket_id = bracket.id
	JOIN dbtable_schema.schedule_bracket_slots slot ON slot.schedule_bracket_id = bracket.id;




		-- can use this to get orphaned items by time
    SELECT service.id
    FROM dbtable_schema.schedule_bracket_services sbs
    JOIN dbtable_schema.services service ON service.id = sbs.service_id
    JOIN dbtable_schema.service_tiers tier ON tier.service_id = service.id
    JOIN dbtable_schema.quotes quote ON quote.service_tier_id = tier.id
    JOIN dbtable_schema.schedule_bracket_slots slot ON slot.id = quote.schedule_bracket_slot_id
    LEFT JOIN dbtable_schema.bookings booking ON booking.quote_id = quote.id
    WHERE
    TIMEZONE('America/Los_Angeles', (DATE_TRUNC('week', quote.slot_date::DATE) + slot.start_time)::TIMESTAMP) < TIMEZONE('America/Los_Angeles', NOW())
    AND booking.id IS NOT NULL
    UNION
    SELECT service.id
    FROM dbtable_schema.schedule_bracket_services sbs
    JOIN dbtable_schema.services service ON service.id = sbs.service_id
    JOIN dbtable_schema.service_tiers tier ON tier.service_id = service.id
    LEFT JOIN dbtable_schema.quotes quote ON quote.service_tier_id = tier.id
    JOIN dbtable_schema.schedule_bracket_slots slot ON slot.id = quote.schedule_bracket_slot_id
    WHERE
    TIMEZONE('America/Los_Angeles', (DATE_TRUNC('week', quote.slot_date::DATE) + slot.start_time)::TIMESTAMP) < TIMEZONE('America/Los_Angeles', NOW())
    AND quote.id IS NOT NULL






    ------------------------------



    -- Locking example

	CREATE OR REPLACE FUNCTION enforce_photo_count() RETURNS trigger AS $$
	DECLARE
			max_photo_count INTEGER := 10;
			photo_count INTEGER := 0;
			must_check BOOLEAN := false;
	BEGIN
			IF TG_OP = 'INSERT' THEN
					must_check := true;
			END IF;

			IF TG_OP = 'UPDATE' THEN
					IF (NEW.owner != OLD.owner) THEN
							must_check := true;
					END IF;
			END IF;

			IF must_check THEN
					-- prevent concurrent inserts from multiple transactions
					LOCK TABLE photos IN EXCLUSIVE MODE;

					SELECT INTO photo_count COUNT(*) 
					FROM photos 
					WHERE owner = NEW.owner;

					IF photo_count >= max_photo_count THEN
							RAISE EXCEPTION 'Cannot insert more than % photos for each user.', max_photo_count;
					END IF;
			END IF;

			RETURN NEW;
	END;
	$$ LANGUAGE plpgsql;


	CREATE TRIGGER enforce_photo_count 
			BEFORE INSERT OR UPDATE ON photos
			FOR EACH ROW EXECUTE PROCEDURE enforce_photo_count();






  const trackStream = (mediaStream: MediaStream) => {

    const mediaRecorder = new MediaRecorder(mediaStream);
    const chunks: BlobPart[] = [];

    // Listen for dataavailable event to obtain the recorded data
    mediaRecorder.addEventListener('dataavailable', (event: BlobEvent) => {
      chunks.push(event.data);
    });

    // Set the recording duration to 10 seconds
    const RECORDING_DURATION_MS = 10000;
    mediaRecorder.start(RECORDING_DURATION_MS);

    const stopChecking = function (lang: string) {
      Object.keys(potentialLanguages).forEach(pl => {
        if (pl !== lang) {
          potentialLanguages[pl].stop();
          delete potentialLanguages[pl];
        } else {
          console.log(potentialLanguages[lang]);
          speechRecognizer.current = potentialLanguages[lang];
        }
      })
    }

    const langs = ['en', 'en-US', 'en-UK', 'de', 'es'];

    const potentialLanguages: Record<string, SpeechRecognition> = langs.reduce((m, l) => {


      const newLang = new window.webkitSpeechRecognition();
      newLang.maxAlternatives = 5;
      newLang.continuous = true;
      newLang.interimResults = true;
      newLang.lang = l;

      newLang.addEventListener('result', (event: SpeechRecognitionEvent) => {
        const lastResult = event.results[event.results.length - 1];
        const { transcript } = lastResult[0];
  
        const isFinal = lastResult.isFinal;
  
        isFinal && console.log({ l, transcript })

        // Check if the user is speaking or not
        if (enableRace.current && isFinal && transcript.length) {
          enableRace.current = false;
          console.log('got a transcript from ', l, transcript);
          stopChecking(l);
          let target = '';
          switch (l) {
            case 'en':
              target = 'English';
              break;
            case 'en-US':
              target = 'English';
              break;
            case 'en-UK':
              target = 'English';
              break;
            case 'de':
              target = 'German';
              break;
            case 'es':
              target = 'Spanish';
              break;
          }

          console.log({ target })

          getPrompt({ id: IPrompts.TRANSLATE, prompt: `${target}|Chinese Simplified|${transcript}`}).unwrap().then(({ promptResult: [translated] }) => {
            sendMessage('text', { style: 'utterance', message: translated.split('Translation:')[1] });
          }).catch(console.error);
          getPrompt({ id: IPrompts.TRANSLATE, prompt: `${target}|English|${transcript}`}).unwrap().then(({ promptResult: [translated] }) => {
            sendMessage('text', { style: 'utterance', message: translated.split('Translation:')[1] });
          }).catch(console.error);
          // sendMessage('text', { style: 'utterance', message: transcript });
        }
      });

      return {
        ...m,
        [l]: newLang
      }
    }, {});

    for (const lang of langs) {
      potentialLanguages[lang].start();
    }

  };